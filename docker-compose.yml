version: '3.8'

services:
  server:
    container_name: ${SERVER_HOST}
    hostname: ${SERVER_HOST}
    build:
      context: ./server
      dockerfile: Dockerfile
    volumes:
      - ./server:/usr/src/app:cached
      - /usr/src/app/node_modules
    ports:
      - "${SERVER_PORT}:${SERVER_PORT}"
    depends_on:
      - db
    environment:
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
      DB_NAME: ${DB_NAME}
      DB_PORT: ${DB_PORT}
      DB_HOST: ${DB_HOST}
    networks:
      - backend-network

  web:
    container_name: ${WEB_HOST}
    hostname: ${WEB_HOST}
    build:
      context: ./web
      dockerfile: Dockerfile
    volumes:
      - ./web:/usr/src/app:cached
      - /usr/src/app/node_modules
    ports:
      - "${WEB_PORT}:3000"
    depends_on:
      - server
      - ollama-webui
    networks:
      - backend-network

  db:
    container_name: ${DB_HOST}
    hostname: ${DB_HOST}
    build:
      context: ./db
      dockerfile: Dockerfile
    ports:
      - "${DB_PORT}:${DB_PORT}"
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_HOST_AUTH_METHOD: trust
    volumes:
      - db_data:/var/lib/postgresql/data
    networks:
      - backend-network

  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    pull_policy: always
    tty: true
    restart: unless-stopped
    ports:
      - ${OLLAMA_PORT}:11434
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=all  # Use all available GPUs
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility  # Expose all relevant GPU driver capabilities
    volumes:
      - ./ollama/ollama:/root/.ollama
    networks:
      - backend-network
    runtime: 'nvidia'
    deploy:
      resources:
        limits:
          cpus: '8.0'  # Allows the container to use all available CPUs
          memory: '20G'  # Allows the container to use all available memory (adjust if necessary)
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ${OLLAMA_WEB_UI_HOST}
    hostname: ${OLLAMA_WEB_UI_HOST}
    depends_on:
      - db
      - ollama
    ports:
      - ${OLLAMA_WEB_UI_PORT}:8080
    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      - OLLAMA_BASE_URLS=http://host.docker.internal:7869 #comma separated ollama hosts
      - ENV=dev
      - WEBUI_NAME=valiantlynx AI
      - WEBUI_URL=http://localhost:${OLLAMA_WEB_UI_PORT}
      - WEBUI_SECRET_KEY=t0p-s3cr3t
      - DATABASE_URL=${DATABASE_URL}
    volumes:
      - ./ollama/ollama-webui:/app/backend/data
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - backend-network

  nginx:
    build:
      context: ./proxy
      dockerfile: Dockerfile
    volumes:
      - ./proxy/cert:/etc/nginx/certificates
    ports:
      - "${NGINX_HTTP_PORT}:${NGINX_HTTP_PORT}"
      - "${NGINX_HTTPS_PORT}:${NGINX_HTTPS_PORT}"
    environment:
      - SERVER_HOST=${SERVER_HOST}
      - SERVER_PORT=${SERVER_PORT}
      - WEB_HOST=${WEB_HOST}
      - WEB_PORT=${WEB_PORT}
      - OLLAMA_WEB_UI_HOST=${OLLAMA_WEB_UI_HOST}
      - OLLAMA_WEB_UI_PORT=${OLLAMA_WEB_UI_PORT}
    depends_on:
      - web
    networks:
      - backend-network

networks:
  backend-network: # Define custom network
    driver: bridge

volumes:
  db_data:
